import keras
from keras.models import Sequential
from keras.layers import Input, Dense, LSTM, GRU, merge
from keras.models import Model
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.embeddings import Embedding
from keras.layers.local import LocallyConnected2D
from keras.preprocessing import sequence
from keras.layers.core import Reshape, Dropout, Activation, Lambda
from keras.regularizers import Regularizer
from keras.layers.normalization import BatchNormalization
from keras.initializations import glorot_normal, get_fans

import tensorflow as tf

from keras.constraints import NonNeg

import keras.backend as K

from keras.engine.topology import Layer
import numpy as np

class DistilTempLayer(Layer):
    def __init__(self, temperature, **kwargs):
        self.temperature = temperature
        self.uses_learning_phase = True
        super(DistilTempLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.trainable_weights = []

    def call(self, x, mask=None):
        if self.temperature != 1.0:
            return K.in_train_phase(x/self.temperature, x)
        else:
            return x

    def get_output_shape_for(self, input_shape):
        return input_shape

class NonNegWeightRegularizer(Regularizer):
    def __init__(self, l1=0., l2=0.):
        self.l1 = K.cast_to_floatx(l1)
        self.l2 = K.cast_to_floatx(l2)
        self.uses_learning_phase = True

    def set_param(self, p):
        self.p = (p - K.abs(p)) / 2

    def __call__(self, loss):
        if not hasattr(self, 'p'):
            raise Exception('Need to call `set_param` on '
                            'WeightRegularizer instance '
                            'before calling the instance. '
                            'Check that you are not passing '
                            'a WeightRegularizer instead of an '
                            'ActivityRegularizer '
                            '(i.e. activity_regularizer="l2" instead '
                            'of activity_regularizer="activity_l2".')
        regularized_loss = loss
        if self.l1:
            regularized_loss += K.sum(self.l1 * K.abs(self.p))
        if self.l2:
            regularized_loss += K.sum(self.l2 * K.square(self.p))
        self.regularized_loss = regularized_loss
        #return K.in_train_phase(regularized_loss, loss)

        #We want this loss in validation too
        return regularized_loss

    def get_config(self):
        return {'name': self.__class__.__name__,
                'l1': float(self.l1),
                'l2': float(self.l2)}

class NonNegActivityRegularizer(Regularizer):

    def __init__(self, l1=0., l2=0.):
        self.l1 = K.cast_to_floatx(l1)
        self.l2 = K.cast_to_floatx(l2)
        self.uses_learning_phase = True
        self.layer = None

    def set_layer(self, layer):
        if self.layer is not None:
            raise Exception('Regularizers cannot be reused')
        self.layer = layer

    def __call__(self, loss):
        if self.layer is None:
            raise Exception('Need to call `set_layer` on '
                            'ActivityRegularizer instance '
                            'before calling the instance.')
        regularized_loss = loss
        for i in range(len(self.layer.inbound_nodes)):
            output = self.layer.get_output_at(i)
            negpart = (output - K.abs(output)) / 2
            if self.l1:
                regularized_loss += K.sum(self.l1 * K.abs(negpart))
            if self.l2:
                regularized_loss += K.sum(self.l2 * K.square(negpart))
        return K.in_train_phase(regularized_loss, loss)

    def get_config(self):
        return {'name': self.__class__.__name__,
                'l1': float(self.l1),
                'l2': float(self.l2)}

def ndims(tensor):
  if K.is_sparse(tensor):
    return tensor.shape.get_shape()[0]
  else:
    return len(tensor.get_shape()._dims)

def expand_idxs(idxs, dim, shape):
  ones = tf.ones(dtype=idxs.dtype, shape=shape)
  return tf.concat(dim, [idxs, ones])

def expand(tensor):
  rank = ndims(tensor)
  if K.is_sparse(tensor):
    new_shape = expand_idxs(tf.shape(tensor.indices)[0:1], 0, (1,))

    new_idxs = expand_idxs(tensor.indices, rank-1, new_shape)
    new_shape = expand_idxs(tensor.shape, 0, (1,))
    return tf.SparseTensor(indices=new_idxs, values=tensor.values, shape=new_shape)
  else:
    return tf.expand_dims(tensor, rank)

def get_nonzero_vals(tensor):
  if K.is_sparse(tensor):
    return tensor.values
  else:
    return tensor

def mult_vals(tensor, weight, columns):
  if isinstance(tensor, tf.SparseTensor):
    slices = []
    flattened =tf.reshape(weight, [-1])
    for i in range(columns):
      orig_slice = tensor.indices[:, 1]
      
      sliced = orig_slice * columns + i
      slices.append(tensor.values * tf.gather(flattened, sliced))
    return tf.concat(0, slices)
  else:
    return tf.expand_dims(tensor, ndims(tensor)) * weight

class NonNegActivityRegularizer2(Regularizer):

    def __init__(self, l1=0., l2=0.):
        self.l1 = K.cast_to_floatx(l1)
        self.l2 = K.cast_to_floatx(l2)
        self.uses_learning_phase = True
        self.layer = None

    def set_layer(self, layer):
        self.layer = layer

    def __call__(self, loss):
        if not hasattr(self, 'layer'):
            raise Exception('Need to call `set_layer` on '
                            'ActivityRegularizer instance '
                            'before calling the instance.')
        weight = self.layer.W
        regularized_loss = loss
        for i in range(len(self.layer.inbound_nodes)):
            input = self.layer.get_input_at(i)
            output = mult_vals(input, weight, self.layer.output_dim)
            negpart = (output - K.abs(output)) / 2
            if self.l1:
                regularized_loss += K.sum(self.l1 * K.abs(negpart))
            if self.l2:
                regularized_loss += K.sum(self.l2 * K.square(negpart))

        return K.in_train_phase(regularized_loss, loss)

    def get_config(self):
        return {'name': self.__class__.__name__,
                'l1': self.l1,
                'l2': self.l2}

def nonneg_init(shape, name, dim_ordering='th'):
    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)
    s = np.sqrt(2. / (fan_in + fan_out))
    return K.variable(np.abs(np.random.normal(loc=0.0, scale=s, size=shape)),
                      name=name)

def model(l1=0.0, l2=0.0, hard_constraint=False, code_network=False, sparse=True, distil_temp=1.0,
                restriction='weight', two_outs=False):    
    manifest_inputs = 185729 + 72 + 6379 + 3812 + 4513 + 33222
    code_inputs = 310488 + 315 + 733 + 70

    def W_regularizer():
        if restriction == 'weights' and l1 > 0.0 or l2 > 0.0:
            return NonNegWeightRegularizer(l1=l1, l2=l2)
        else:
            return None

    def A_regularizer():
        if restriction == 'activations' and l1 > 0.0 or l2 > 0.0:
            return NonNegActivityRegularizer(l1=l1, l2=l2)
        if restriction == 'presum' and l1 > 0.0 or l2 > 0.0:
            return NonNegActivityRegularizer2(l1=l1, l2=l2)
        else:
            return None

    if hard_constraint:
        W_constraint = NonNeg()
    else:
        W_constraint = None

    def init():
        do_nonneg = False
        if do_nonneg:
            return nonneg_init
        else:
            return glorot_normal

    manifest_tensor = tf.sparse_placeholder(tf.float32)
    code_tensor = tf.sparse_placeholder(tf.float32)
    
    manifest_input = Input(shape=(manifest_inputs,), sparse=sparse)
    code_input = Input(shape=(code_inputs,), sparse=sparse)

    all_inputs = merge([manifest_input, code_input], mode='concat', concat_axis=-1)
    
    hidden = Dense(200, init=init(), activation='relu', W_constraint=W_constraint, W_regularizer=W_regularizer(), activity_regularizer=A_regularizer())(all_inputs)    
    
    hidden = Dropout(0.5)(hidden)
    last = Dense(200, init=init(), activation='relu', W_constraint=W_constraint, W_regularizer=W_regularizer(), activity_regularizer=A_regularizer())(hidden)
    
    if code_network:
        code = Dense(200, activation='relu')(code_input)
            
        code = Dense(200, activation='relu')(code)
        
        last = merge([code, last], mode='concat')

    last = Dropout(0.5)(last)
    if distil_temp != 1.0 or two_outs:
      last = Dense(2, init=init(), W_constraint=W_constraint, W_regularizer=W_regularizer(), activity_regularizer=A_regularizer())(last)

      last = DistilTempLayer(distil_temp)(last)

      predictions = Activation('softmax')(last)

    #Slice off the second class to not have to rewrite my data code
      predictions = Lambda(lambda x: x[:, 0:1], name='prediction')(predictions)
    else:
      predictions = Dense(1, W_constraint=W_constraint, W_regularizer=W_regularizer(), activity_regularizer=A_regularizer())(last)

    m = Model(input=[manifest_input, code_input], output=predictions)
    
    return m

def as_dense(m):    
    manifest_inputs = 185729 + 72 + 6379 + 3812 + 4513 + 33222
    code_inputs = 310488 + 315 + 733 + 70

    manifest_input = Input(shape=(manifest_inputs,), sparse=False)
    code_input = Input(shape=(code_inputs,), sparse=False)

    predictions = m([manifest_input, code_input])

    return Model(input=[manifest_input, code_input], output=predictions)
